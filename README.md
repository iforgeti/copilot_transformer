# copilot_transformer

what new from previous code copilot project (LSTM)
* change tokenizer from spacy to hugging face tokenizer (based on the codeparrot/codeparrot-small pre-trained model)
* used only decoder part from transfromers architecture

other detail

* train for 5 hr
* dataset from huggingface-course/codeparrot-ds-train (used just 2.5 % from full datasets)

## Review

![image](https://user-images.githubusercontent.com/78832408/224545894-db24cbd3-99d0-435c-b1ec-8916d8fc32a1.png)

---

## note
- not sure about greedy and beam search 

